# 混合检索功能配置说明

## 功能概述

本系统实现了完整的混合检索功能，包括：

1. **BM25检索** - 基于关键词的精确匹配
2. **向量检索** - 基于语义的相似度匹配
3. **混合检索** - BM25 + 向量检索的融合
4. **重排序模型** - 使用BGE或Cohere Rerank模型优化结果
5. **多路召回** - 通过查询改写实现多路召回
6. **结果融合** - 智能融合多路检索结果

## 配置说明

### 1. 环境变量配置

在 `.env.local` 文件中添加以下配置：

```bash
# 检索配置
USE_HYBRID_SEARCH=true
USE_RERANK=false
USE_QUERY_REWRITE=false
NUM_PATHS=3

# BM25和向量检索权重
BM25_WEIGHT=0.3
VECTOR_WEIGHT=0.7

# 重排序模型配置
RERANK_MODEL=BAAI/bge-reranker-v2-m3

# Cohere Rerank配置（如果使用Cohere）
COHERE_API_KEY=your_cohere_api_key
```

### 2. 配置参数说明

| 参数 | 说明 | 默认值 | 可选值 |
|------|------|--------|--------|
| `USE_HYBRID_SEARCH` | 是否启用混合检索 | `true` | `true`, `false` |
| `USE_RERANK` | 是否启用重排序 | `false` | `true`, `false` |
| `USE_QUERY_REWRITE` | 是否启用查询改写 | `false` | `true`, `false` |
| `NUM_PATHS` | 多路召回路径数 | `3` | `1-10` |
| `BM25_WEIGHT` | BM25检索权重 | `0.3` | `0.0-1.0` |
| `VECTOR_WEIGHT` | 向量检索权重 | `0.7` | `0.0-1.0` |
| `RERANK_MODEL` | 重排序模型名称 | `BAAI/bge-reranker-v2-m3` | 模型名称 |

### 3. 检索模式

#### 模式1：纯向量检索（默认）

```bash
USE_HYBRID_SEARCH=false
USE_RERANK=false
```

**特点：**
- 只使用向量相似度检索
- 速度快，资源占用少
- 适合语义查询

#### 模式2：混合检索（推荐）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=false
```

**特点：**
- BM25 + 向量检索融合
- 兼顾精确匹配和语义匹配
- 检索质量显著提升

#### 模式3：混合检索 + 重排序（最佳质量）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=true
USE_QUERY_REWRITE=false
```

**特点：**
- 混合检索 + BGE重排序
- 检索质量最高
- 需要额外资源

#### 模式4：完整功能（最高质量）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=true
USE_QUERY_REWRITE=true
NUM_PATHS=3
```

**特点：**
- 混合检索 + 重排序 + 查询改写
- 多路召回和融合
- 检索质量最优
- 资源占用最大

## 使用方法

### 1. 安装依赖

```bash
# 基础依赖
pip install sentence-transformers

# 如果使用Cohere Rerank
pip install cohere
```

### 2. 代码中使用

```python
from src.core.rag_engine import RAGEngine
from src.core.vector_store import ChromaVectorStore
from src.core.llm import AlibabaLLM

# 创建向量存储
vector_store = ChromaVectorStore(
    collection_name="my_collection",
    embedding_function=embedding_function
)

# 创建LLM
llm = AlibabaLLM()

# 创建RAG引擎（启用混合检索）
rag_engine = RAGEngine(
    vector_store=vector_store,
    llm=llm,
    use_hybrid_search=True,
    use_rerank=False,
    use_query_rewrite=False,
    num_paths=3
)

# 查询
documents, retrieval_time = rag_engine.retrieve(
    query="什么是企业知识库？",
    top_k=4,
    score_threshold=0.7
)
```

### 3. API中使用

系统会自动根据配置使用相应的检索模式，无需额外配置。

## 性能对比

| 检索模式 | 检索质量 | 响应时间 | 资源占用 |
|----------|----------|----------|----------|
| 纯向量检索 | ⭐⭐⭐ | 快 | 低 |
| 混合检索 | ⭐⭐⭐⭐ | 中等 | 中等 |
| 混合+重排序 | ⭐⭐⭐⭐⭐ | 慢 | 高 |
| 完整功能 | ⭐⭐⭐⭐⭐ | 很慢 | 很高 |

## 推荐配置

### 小规模部署（< 10万文档）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=false
BM25_WEIGHT=0.3
VECTOR_WEIGHT=0.7
```

### 中等规模部署（10万-100万文档）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=true
USE_QUERY_REWRITE=false
NUM_PATHS=2
```

### 大规模部署（> 100万文档）

```bash
USE_HYBRID_SEARCH=true
USE_RERANK=true
USE_QUERY_REWRITE=true
NUM_PATHS=3
```

## 注意事项

1. **首次查询较慢**：BM25索引需要初始化，首次查询会较慢
2. **内存占用**：重排序模型需要额外内存，建议至少8GB
3. **模型下载**：首次使用会自动下载BGE模型，约400MB
4. **网络要求**：如果使用Cohere API，需要稳定的网络连接
5. **权重调整**：根据实际效果调整BM25和向量检索的权重

## 故障排查

### 问题1：重排序模型加载失败

**错误信息：** `初始化重排序模型失败`

**解决方案：**
```bash
pip install sentence-transformers
```

### 问题2：混合检索效果不佳

**可能原因：**
- BM25权重设置不当
- 文档预处理不充分

**解决方案：**
调整权重配置：
```bash
BM25_WEIGHT=0.4
VECTOR_WEIGHT=0.6
```

### 问题3：查询改写效果不好

**可能原因：**
- LLM模型能力不足
- 查询改写参数设置不当

**解决方案：**
1. 使用更强的LLM模型
2. 减少`NUM_PATHS`数量
3. 关闭查询改写功能

## 代码位置

- **混合检索器：** `src/core/hybrid_retriever.py`
- **重排序模型：** `src/core/reranker.py`
- **RAG引擎：** `src/core/rag_engine.py`
- **配置文件：** `src/config/settings.py`

## 更新日志

### v1.0.0 (2024-01-30)
- ✅ 实现BM25检索
- ✅ 实现混合检索策略
- ✅ 集成BGE重排序模型
- ✅ 实现查询改写功能
- ✅ 实现多路召回和融合
- ✅ 添加配置参数